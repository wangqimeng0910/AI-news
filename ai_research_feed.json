{
  "generated_at": "2025-12-19T02:51:03.083212+00:00",
  "count": 25,
  "items": [
    {
      "source_id": "openai_news",
      "source_name": "OpenAI News",
      "category": "company",
      "title": "Evaluating chain-of-thought monitorability",
      "link": "https://openai.com/index/evaluating-chain-of-thought-monitorability",
      "published": "2025-12-18T12:00:00+00:00",
      "summary": "OpenAI introduces a new framework and evaluation suite for chain-of-thought monitorability, covering 13 evaluations across 24 environments. Our findings show that monitoring a model’s internal reasoning is far more effective than monitoring outputs alone, offering a promising path toward scalable control as AI systems grow more capable."
    },
    {
      "source_id": "openai_news",
      "source_name": "OpenAI News",
      "category": "company",
      "title": "Updating our Model Spec with teen protections",
      "link": "https://openai.com/index/updating-model-spec-with-teen-protections",
      "published": "2025-12-18T11:00:00+00:00",
      "summary": "OpenAI is updating its Model Spec with new Under-18 Principles that define how ChatGPT should support teens with safe, age-appropriate guidance grounded in developmental science. The update strengthens guardrails, clarifies expected model behavior in higher-risk situations, and builds on our broader work to improve teen safety across ChatGPT."
    },
    {
      "source_id": "openai_news",
      "source_name": "OpenAI News",
      "category": "company",
      "title": "AI literacy resources for teens and parents",
      "link": "https://openai.com/index/ai-literacy-resources-for-teens-and-parents",
      "published": "2025-12-18T11:00:00+00:00",
      "summary": "OpenAI shares new AI literacy resources to help teens and parents use ChatGPT thoughtfully, safely, and with confidence. The guides include expert-vetted tips for responsible use, critical thinking, healthy boundaries, and supporting teens through emotional or sensitive topics."
    },
    {
      "source_id": "openai_news",
      "source_name": "OpenAI News",
      "category": "company",
      "title": "Deepening our collaboration with the U.S. Department of Energy",
      "link": "https://openai.com/index/us-department-of-energy-collaboration",
      "published": "2025-12-18T11:00:00+00:00",
      "summary": "OpenAI and the U.S. Department of Energy have signed a memorandum of understanding to deepen collaboration on AI and advanced computing in support of scientific discovery. The agreement builds on ongoing work with national laboratories and helps establish a framework for applying AI to high-impact research across the DOE ecosystem."
    },
    {
      "source_id": "openai_news",
      "source_name": "OpenAI News",
      "category": "company",
      "title": "Introducing GPT-5.2-Codex",
      "link": "https://openai.com/index/introducing-gpt-5-2-codex",
      "published": "2025-12-18T00:00:00+00:00",
      "summary": "GPT-5.2-Codex is OpenAI’s most advanced coding model, offering long-horizon reasoning, large-scale code transformations, and enhanced cybersecurity capabilities."
    },
    {
      "source_id": "deepmind_news",
      "source_name": "Google DeepMind",
      "category": "company",
      "title": "Watch a podcast discussion about Gemini 3 and the future of Search.",
      "link": "https://blog.google/technology/ai/release-notes-podcast-search/",
      "published": "2025-12-18T00:00:00+00:00",
      "summary": "Learn how Gemini 3 powers Google Search with Generative UI, Nano Banana, and interactive graphics."
    },
    {
      "source_id": "deepmind_news",
      "source_name": "Google DeepMind",
      "category": "company",
      "title": "Gemini 3 Flash: frontier intelligence built for speed",
      "link": "https://blog.google/products/gemini/gemini-3-flash/",
      "published": "2025-12-17T16:00:00+00:00",
      "summary": "Gemini 3 Flash text"
    },
    {
      "source_id": "deepmind_news",
      "source_name": "Google DeepMind",
      "category": "company",
      "title": "The latest AI news we announced in November",
      "link": "https://blog.google/technology/ai/google-ai-updates-november-2025/",
      "published": "2025-12-05T19:45:00+00:00",
      "summary": "mp4 showing a carousel of images with text like \"Gemini 3\" \"Nano Banana Pro\" and \"Help me plan a trip\""
    },
    {
      "source_id": "deepmind_news",
      "source_name": "Google DeepMind",
      "category": "company",
      "title": "Watch ‘The Thinking Game,’ a documentary about Google DeepMind, for free on YouTube.",
      "link": "https://blog.google/technology/google-deepmind/the-thinking-game/",
      "published": "2025-11-25T16:00:00+00:00",
      "summary": "We’re bringing “The Thinking Game\" to the Google DeepMind YouTube channel."
    },
    {
      "source_id": "deepmind_news",
      "source_name": "Google DeepMind",
      "category": "company",
      "title": "How we’re bringing AI image verification to the Gemini app",
      "link": "https://blog.google/technology/ai/ai-image-verification-gemini-app/",
      "published": "2025-11-20T15:00:00+00:00",
      "summary": "Golden retriever catching frisbee over water. UI overlay shows Google AI watermark detected."
    },
    {
      "source_id": "arxiv_cs_lg",
      "source_name": "arXiv cs.LG (Machine Learning)",
      "category": "arxiv",
      "title": "LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts",
      "link": "https://arxiv.org/abs/2512.14706",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14706v1 Announce Type: new Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders..."
    },
    {
      "source_id": "arxiv_cs_lg",
      "source_name": "arXiv cs.LG (Machine Learning)",
      "category": "arxiv",
      "title": "Autonomous Source Knowledge Selection in Multi-Domain Adaptation",
      "link": "https://arxiv.org/abs/2512.14710",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14710v1 Announce Type: new Abstract: Unsupervised multi-domain adaptation plays a key role in transfer learning by leveraging acquired rich source information from multiple source domains to solve target task from an unlabeled target domain. However, multiple source domains often contain much redundant or unrelated information which can harm transfer performance, especially when in mass..."
    },
    {
      "source_id": "arxiv_cs_lg",
      "source_name": "arXiv cs.LG (Machine Learning)",
      "category": "arxiv",
      "title": "SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI",
      "link": "https://arxiv.org/abs/2512.14712",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14712v1 Announce Type: new Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for s..."
    },
    {
      "source_id": "arxiv_cs_lg",
      "source_name": "arXiv cs.LG (Machine Learning)",
      "category": "arxiv",
      "title": "A Bayesian latent class reinforcement learning framework to capture adaptive, feedback-driven travel behaviour",
      "link": "https://arxiv.org/abs/2512.14713",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14713v1 Announce Type: new Abstract: Many travel decisions involve a degree of experience formation, where individuals learn their preferences over time. At the same time, there is extensive scope for heterogeneity across individual travellers, both in their underlying preferences and in how these evolve. The present paper puts forward a Latent Class Reinforcement Learning (LCRL) model ..."
    },
    {
      "source_id": "arxiv_cs_lg",
      "source_name": "arXiv cs.LG (Machine Learning)",
      "category": "arxiv",
      "title": "Improving Underwater Acoustic Classification Through Learnable Gabor Filter Convolution and Attention Mechanisms",
      "link": "https://arxiv.org/abs/2512.14714",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14714v1 Announce Type: new Abstract: Remotely detecting and classifying underwater acoustic targets is critical for environmental monitoring and defence. However, the complex nature of ship-radiated and environmental underwater noise poses significant challenges to accurate signal processing. While recent advancements in machine learning have improved classification accuracy, issues suc..."
    },
    {
      "source_id": "arxiv_cs_cl",
      "source_name": "arXiv cs.CL (Computation and Language)",
      "category": "arxiv",
      "title": "Incentives or Ontology? A Structural Rebuttal to OpenAI's Hallucination Thesis",
      "link": "https://arxiv.org/abs/2512.14801",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14801v1 Announce Type: new Abstract: OpenAI has recently argued that hallucinations in large language models result primarily from misaligned evaluation incentives that reward confident guessing rather than epistemic humility. On this view, hallucination is a contingent behavioral artifact, remediable through improved benchmarks and reward structures. In this paper, we challenge that in..."
    },
    {
      "source_id": "arxiv_cs_cl",
      "source_name": "arXiv cs.CL (Computation and Language)",
      "category": "arxiv",
      "title": "T5Gemma 2: Seeing, Reading, and Understanding Longer",
      "link": "https://arxiv.org/abs/2512.14856",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14856v1 Announce Type: new Abstract: We introduce T5Gemma 2, the next generation of the T5Gemma family of lightweight open encoder-decoder models, featuring strong multilingual, multimodal and long-context capabilities. T5Gemma 2 follows the adaptation recipe (via UL2) in T5Gemma -- adapting a pretrained decoder-only model into an encoder-decoder model, and extends it from text-only reg..."
    },
    {
      "source_id": "arxiv_cs_cl",
      "source_name": "arXiv cs.CL (Computation and Language)",
      "category": "arxiv",
      "title": "Integrating Large Language Models and Knowledge Graphs to Capture Political Viewpoints in News Media",
      "link": "https://arxiv.org/abs/2512.14887",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14887v1 Announce Type: new Abstract: News sources play a central role in democratic societies by shaping political and social discourse through specific topics, viewpoints and voices. Understanding these dynamics is essential for assessing whether the media landscape offers a balanced and fair account of public debate. In earlier work, we introduced a pipeline that, given a news corpus,..."
    },
    {
      "source_id": "arxiv_cs_cl",
      "source_name": "arXiv cs.CL (Computation and Language)",
      "category": "arxiv",
      "title": "DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline",
      "link": "https://arxiv.org/abs/2512.14896",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14896v1 Announce Type: new Abstract: Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy. Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured..."
    },
    {
      "source_id": "arxiv_cs_cl",
      "source_name": "arXiv cs.CL (Computation and Language)",
      "category": "arxiv",
      "title": "Multiscale Aggregated Hierarchical Attention (MAHA): A Game Theoretic and Optimization Driven Approach to Efficient Contextual Modeling in Large Language Models",
      "link": "https://arxiv.org/abs/2512.14925",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.14925v1 Announce Type: new Abstract: The quadratic computational complexity of MultiHead SelfAttention (MHSA) remains a fundamental bottleneck in scaling Large Language Models (LLMs) for longcontext tasks. While sparse and linearized attention mechanisms attempt to mitigate this, they often compromise the representation of global dependencies or fail to capture multiscale semantic granu..."
    },
    {
      "source_id": "arxiv_cs_ai",
      "source_name": "arXiv cs.AI (Artificial Intelligence)",
      "category": "arxiv",
      "title": "Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records",
      "link": "https://arxiv.org/abs/2512.13700",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.13700v1 Announce Type: new Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally dep..."
    },
    {
      "source_id": "arxiv_cs_ai",
      "source_name": "arXiv cs.AI (Artificial Intelligence)",
      "category": "arxiv",
      "title": "Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference",
      "link": "https://arxiv.org/abs/2512.13701",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.13701v1 Announce Type: new Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectori..."
    },
    {
      "source_id": "arxiv_cs_ai",
      "source_name": "arXiv cs.AI (Artificial Intelligence)",
      "category": "arxiv",
      "title": "Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents",
      "link": "https://arxiv.org/abs/2512.13704",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.13704v1 Announce Type: new Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correctin..."
    },
    {
      "source_id": "arxiv_cs_ai",
      "source_name": "arXiv cs.AI (Artificial Intelligence)",
      "category": "arxiv",
      "title": "LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms",
      "link": "https://arxiv.org/abs/2512.13713",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.13713v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \\textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C..."
    },
    {
      "source_id": "arxiv_cs_ai",
      "source_name": "arXiv cs.AI (Artificial Intelligence)",
      "category": "arxiv",
      "title": "AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach",
      "link": "https://arxiv.org/abs/2512.13714",
      "published": "2025-12-18T05:00:00+00:00",
      "summary": "arXiv:2512.13714v1 Announce Type: new Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stab..."
    }
  ]
}